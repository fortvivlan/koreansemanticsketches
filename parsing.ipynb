{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–µ–æ–±—Ö–æ–¥–∏–º–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å git: https://git-scm.com/. –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–¥–æ–π–¥—É—Ç. \n",
    "\n",
    "–°–ª–µ–¥—É—é—â–∏–µ –º–æ–¥—É–ª–∏ –ª—É—á—à–µ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å –∏–º–µ–Ω–Ω–æ –≤ —Ç–∞–∫–æ–º –ø–æ—Ä—è–¥–∫–µ (rnnmorph –∏ PyKoSpacing –∏–±–∞ –∏—Å–ø–æ–ª—å–∑—É—é—Ç tensorflow –∏ keras, –∏ rnnmorph –º–æ–∂–µ—Ç –ø–æ—Å—Ç–∞–≤–∏—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –±–æ–ª–µ–µ –Ω–æ–≤—ã–µ –≤–µ—Ä—Å–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —Ä–∞–±–æ—Ç–∞—é—Ç —Å pykospacing)\n",
    "\n",
    "pip install razdel\n",
    "\n",
    "pip install rnnmorph\n",
    "\n",
    "pip install spacy_udpipe \n",
    "\n",
    "pip install keras==2.4.3\n",
    "\n",
    "pip install git+https://github.com/haven-jeon/PyKoSpacing.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''–ú–æ–¥—É–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å. \n",
    "PyKoSpacing - —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∫–æ—Ä–µ–π—Å–∫–æ–≥–æ; spacy_udpipe - –º–æ—Ä—Ñ–æ–ø–∞—Ä—Å–µ—Ä (–º—ã –∑–∞–≥—Ä—É–∑–∏–º –µ–≥–æ –¥–ª—è –∫–æ—Ä–µ–π—Å–∫–æ–≥–æ); \n",
    "razdel - —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ; rnnmorph - –º–æ—Ä—Ñ–æ–ø–∞—Ä—Å–µ—Ä —Ä—É—Å—Å–∫–æ–≥–æ'''\n",
    "\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from pykospacing import Spacing\n",
    "from razdel import tokenize\n",
    "from rnnmorph.predictor import RNNMorphPredictor\n",
    "import spacy_udpipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader(path, lang):\n",
    "    '''–§—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö'''\n",
    "    data = []\n",
    "    with open(path, 'r', encoding='utf8') as file:  \n",
    "        for line in file:\n",
    "            data.append(json.loads(line))\n",
    "    data = pd.DataFrame(data)\n",
    "    data = data[['tweet', 'language']]  # –æ—Å—Ç–∞–≤–∏–º –≤ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–µ —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏\n",
    "    data = data[data.language == lang]  # –æ—Ç—Å–µ–µ–º —Ç–≤–∏—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ —Å–∞–º —Ç–≤–∏—Ç—Ç–µ—Ä –æ–ø—Ä–µ–¥–µ–ª–∏–ª –∫–∞–∫ –Ω–µ –Ω–∞ –Ω—É–∂–Ω–æ–º –Ω–∞–º —è–∑—ã–∫–µ\n",
    "    data['tweet'] = data['tweet'].apply(lambda tweet: re.sub(r'https://t.co/\\w+', '', tweet))  # —É–¥–∞–ª–∏–º —Å—Å—ã–ª–∫–∏. –ø–æ-—Ö–æ—Ä–æ—à–µ–º—É, —Ç–∞–º –µ—â–µ –Ω–∞–¥–æ —á–∏—Å—Ç–∏—Ç—å, –Ω–æ –Ω–∞–º –Ω–µ —Ç–∞–∫ –ø—Ä–∏–Ω—Ü–∏–ø–∏–∞–ª—å–Ω–æ\n",
    "    data['tweet'] = data['tweet'].apply(lambda tweet: re.sub(r'@[a-zA-Z0-9_-]+', '', tweet))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Î•òÏß±Ïö∞ Í≤® Ï°¥ÎÇòÏïºÌï®</td>\n",
       "      <td>ko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ïú†ÏßÑÏ∞®  „Öú„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã</td>\n",
       "      <td>ko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ï∞∞Îñ°Í∞ôÏùÄÍ±∞ ÎÇòÏò¨ÎïåÍπåÏßÄ ÏãúÎèÑÌñàÍ∏∞ÎïåÎ¨∏Ïù¥Í∏¥ÌïúÎç∞;; ÏïÑÎ¨¥Ìäº ÏÜåÎ¶Ñ „Ñ∑„Ñ∑;;</td>\n",
       "      <td>ko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>„Öà„Ñ¥Ï∞∞Îñ°Í∞ôÏùÄÍ±∞Îßå ÎÇòÏò§ÎÑ§;;</td>\n",
       "      <td>ko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>üêû ÏàòÏù∏ ÌÖåÏä§Ìä∏ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ÍπÄÎûòÎπàÎãòÏùÄ ÏûêÏã†Ïù¥ ÌÜ†ÎÅº ÏàòÏù∏Ïù∏ Í±∏...</td>\n",
       "      <td>ko</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet language\n",
       "0                                         Î•òÏß±Ïö∞ Í≤® Ï°¥ÎÇòÏïºÌï®       ko\n",
       "1                                  Ïú†ÏßÑÏ∞®  „Öú„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã         ko\n",
       "2               Ï∞∞Îñ°Í∞ôÏùÄÍ±∞ ÎÇòÏò¨ÎïåÍπåÏßÄ ÏãúÎèÑÌñàÍ∏∞ÎïåÎ¨∏Ïù¥Í∏¥ÌïúÎç∞;; ÏïÑÎ¨¥Ìäº ÏÜåÎ¶Ñ „Ñ∑„Ñ∑;;       ko\n",
       "3                                     „Öà„Ñ¥Ï∞∞Îñ°Í∞ôÏùÄÍ±∞Îßå ÎÇòÏò§ÎÑ§;;       ko\n",
       "4  üêû ÏàòÏù∏ ÌÖåÏä§Ìä∏ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ÍπÄÎûòÎπàÎãòÏùÄ ÏûêÏã†Ïù¥ ÌÜ†ÎÅº ÏàòÏù∏Ïù∏ Í±∏...       ko"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''–ó–∞–≥—Ä—É–∑–∏–º –¥–∞—Ç–∞—Å–µ—Ç –∏ –ø–µ—Ä–µ–≤–µ–¥–µ–º –µ–≥–æ –≤ —Ñ–æ—Ä–º–∞—Ç panel datas'''\n",
    "\n",
    "korean = loader('korean.json', 'ko')\n",
    "\n",
    "korean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already downloaded a model for the 'ko' language\n"
     ]
    }
   ],
   "source": [
    "'''–ó–∞–≥—Ä—É–∑–∏–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–ª—è —Ä–∞–±–æ—Ç—ã –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã - –º–æ–¥–µ–ª—å –¥–ª—è –∫–æ—Ä–µ–π—Å–∫–æ–≥–æ'''\n",
    "\n",
    "spacy_udpipe.download(\"ko\")\n",
    "nlp = spacy_udpipe.load(\"ko\")\n",
    "spacing = Spacing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: —è—á–µ–π–∫–∏ —Å –∫–æ–¥–æ–º –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∏ –º–æ—Ä—Ñ–æ—Ä–∞–∑–±–æ—Ä–∞ –º–æ–≥—É—Ç **–¥–æ–ª–≥–æ** —Ä–∞–±–æ—Ç–∞—Ç—å. –≠—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ: –ø–æ—á—Ç–∏ –≤—Å–µ –Ω–∞—à–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–∞ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç—è—Ö, –∞ –æ–Ω–∏ –Ω–µ–±—ã—Å—Ç—Ä—ã–µ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''–û—Ç–ø–∞—Ä—Å–∏–º —Ç–≤–∏—Ç—ã: —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –∏—Ö –∏ —Å–¥–µ–ª–∞–µ–º –º–æ—Ä—Ñ–æ—Ä–∞–∑–±–æ—Ä. –¢–µ–ø–µ—Ä—å —É –Ω–∞—Å –±—É–¥—É—Ç —Å–ø–∏—Å–∫–∏ —Ç–æ–∫–µ–Ω–æ–≤ –∏ –ª–µ–º–º'''\n",
    "\n",
    "parsedkorean = []\n",
    "for tweet in korean.tweet:\n",
    "    doc = nlp(spacing(tweet))\n",
    "    parsedtweet = []\n",
    "    for token in doc:\n",
    "        parsedtweet.append((token.text, token.lemma_))\n",
    "    parsedkorean.append(parsedtweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Î•òÏß±Ïö∞', 'Î•òÏß±Ïö∞'), ('Í≤®', 'Í∏∞+Ïñ¥'), ('Ï°¥', 'Ï°¥'), ('ÎÇòÏïºÌï®', 'ÎÇòÏïº+Ìïò+„ÖÅ')],\n",
       " [('Ïú†ÏßÑÏ∞®', 'Ïú†ÏßÑÏ∞®'), ('„Öú„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã', '„Öú„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã')],\n",
       " [('Ï∞∞Îñ°', 'Ï∞∞Îñ°'),\n",
       "  ('Í∞ôÏùÄ', 'Í∞ô+„Ñ¥'),\n",
       "  ('Í±∞', 'Í±∞'),\n",
       "  ('ÎÇòÏò¨', 'ÎÇòÏò§+„Ñπ'),\n",
       "  ('ÎïåÍπåÏßÄ', 'Îïå+ÍπåÏßÄ'),\n",
       "  ('ÏãúÎèÑ', 'Ïãú+ÎèÑ'),\n",
       "  ('ÌñàÍ∏∞', 'Ìïò+Ïóà+Í∏∞'),\n",
       "  ('ÎïåÎ¨∏Ïù¥Í∏¥', 'ÎïåÎ¨∏+Ïù¥+Í∏∞+Îäî'),\n",
       "  ('ÌïúÎç∞', 'ÌïúÎç∞'),\n",
       "  (';', ';'),\n",
       "  (';', ';'),\n",
       "  ('ÏïÑÎ¨¥Ìäº', 'ÏïÑÎ¨¥Ìäº'),\n",
       "  ('ÏÜåÎ¶Ñ', 'ÏÜåÎ¶Ñ'),\n",
       "  ('„Ñ∑„Ñ∑', '„Ñ∑„Ñ∑'),\n",
       "  (';', ';'),\n",
       "  (';', ';')]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''–ú–æ–∂–Ω–æ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å, –∫–∞–∫ —ç—Ç–æ –±—É–¥–µ—Ç –≤—ã–≥–ª—è–¥–µ—Ç—å, –Ω–∞ –ø–µ—Ä–≤—ã—Ö —Ç—Ä–µ—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö'''\n",
    "parsedkorean[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117\n"
     ]
    }
   ],
   "source": [
    "'''–ó–∞–¥–∞–¥–∏–º —Å–ª–æ–≤–æ, –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å –∫–æ—Ç–æ—Ä—ã–º –º—ã —Ö–æ—Ç–∏–º –æ—Ç—Å–µ—è—Ç—å'''\n",
    "\n",
    "keyword = 'Í≤É'  # –º–µ–Ω—è–π—Ç–µ –Ω–∞ –Ω—É–∂–Ω–æ–µ —Å–ª–æ–≤–æ. –≠—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –∏–ª–∏ —Ç–æ–∫–µ–Ω, –∏–ª–∏ –ª–µ–º–º–∞ (–Ω–æ —è –Ω–µ –∑–Ω–∞—é, –∫–∞–∫ –≤ –∫–æ—Ä–µ–π—Å–∫–æ–º —è–∑—ã–∫–µ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä—É–µ—Ç—Å—è, –µ—Å—Ç—å –ª–∏ —É –Ω–∏—Ö –∏–Ω—Ñ–∏–Ω–∏—Ç–∏–≤—ã?)\n",
    "filteredkorean = []\n",
    "for tweet in parsedkorean:\n",
    "    for token in tweet:\n",
    "        if keyword in token:\n",
    "            filteredkorean.append(tweet)\n",
    "            break\n",
    "print(len(filteredkorean))  # –ø–æ—Å–º–æ—Ç—Ä–∏–º, —Å–∫–æ–ª—å–∫–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π —É –Ω–∞—Å –ø–æ–ª—É—á–∏–ª–æ—Å—å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''–ó–∞–ø–∏—à–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤ xlsx'''\n",
    "\n",
    "d = dict(enumerate(filteredkorean))\n",
    "dict_df = pd.DataFrame({key:pd.Series(value) for key, value in d.items()}).T\n",
    "dict_df.to_excel('korean.xlsx')  # –ú–µ–Ω—è–π—Ç–µ –Ω–∞–∑–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–µ–ø–µ—Ä—å —Ç–æ –∂–µ —Å–¥–µ–ª–∞–µ–º –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ—Ä—Ñ–æ–ø–∞—Ä—Å–µ—Ä - –µ—Å–ª–∏ –≤—ã–≤–∞–ª–∏—Ç –∫–∞–∫–∏–µ-—Ç–æ –≤–æ—Ä–Ω–∏–Ω–≥–∏, –Ω–µ –ø—É–≥–∞–π—Ç–µ—Å—å, –æ–Ω –ª—é–±–∏—Ç. –ì–ª–∞–≤–Ω–æ–µ, —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ –æ—à–∏–±–æ–∫ –≤ –Ω–∏–∂–Ω–∏—Ö —è—á–µ–π–∫–∞—Ö'''\n",
    "\n",
    "predictor = RNNMorphPredictor(language=\"ru\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–°–∞–º–∞ 10 –º–µ—Å –∏—Å–∫–∞–ª–∞ —Ä–∞–±–æ—Ç—É. –ó–Ω–∞—é.</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>–ñ–µ–ª–µ–∑–Ω—ã–π –∞—Ä–≥—É–º–µ–Ω—Ç.</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>–¢.–µ. –Ω–∞–∑–ª–æ –º–∞–º–µ –æ—Ç–º–æ—Ä–æ–∂—É —É—à–∏?</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>–≠—Ç–æ –µ–≥–æ –≤—ã–±–æ—Ä.</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–ü—Ä–∏—Å–æ–µ–¥–∏–Ω—è–π—Ç–µ—Å—å –∫–æ –º–Ω–µ –≤ –º–æ–µ–π –ê—É–¥–∏–æ–∫–æ–º–Ω–∞—Ç–µ!</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           tweet language\n",
       "0               –°–∞–º–∞ 10 –º–µ—Å –∏—Å–∫–∞–ª–∞ —Ä–∞–±–æ—Ç—É. –ó–Ω–∞—é.       ru\n",
       "1                             –ñ–µ–ª–µ–∑–Ω—ã–π –∞—Ä–≥—É–º–µ–Ω—Ç.       ru\n",
       "2                  –¢.–µ. –Ω–∞–∑–ª–æ –º–∞–º–µ –æ—Ç–º–æ—Ä–æ–∂—É —É—à–∏?       ru\n",
       "3                                 –≠—Ç–æ –µ–≥–æ –≤—ã–±–æ—Ä.       ru\n",
       "4  –ü—Ä–∏—Å–æ–µ–¥–∏–Ω—è–π—Ç–µ—Å—å –∫–æ –º–Ω–µ –≤ –º–æ–µ–π –ê—É–¥–∏–æ–∫–æ–º–Ω–∞—Ç–µ!         ru"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "russian = loader('russian.json', 'ru')\n",
    "russian.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ë—É–¥—å—Ç–µ –≥–æ—Ç–æ–≤—ã –∫ —Ç–æ–º—É, —á—Ç–æ RNNMorph **–æ—á–µ–Ω—å** –º–µ–¥–ª–µ–Ω–Ω—ã–π. –ú–æ–∂–Ω–æ –∏–¥—Ç–∏ –ø–∏—Ç—å —á–∞–π..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsedrussian = []\n",
    "for tweet in russian.tweet:\n",
    "    parsedrussian.append([(w.word, w.normal_form) for w in predictor.predict([t.text for t in tokenize(tweet)])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('–°–∞–º–∞', '—Å–∞–º'),\n",
       "  ('10', '10'),\n",
       "  ('–º–µ—Å', '–º–µ—Å'),\n",
       "  ('–∏—Å–∫–∞–ª–∞', '–∏—Å–∫–∞—Ç—å'),\n",
       "  ('—Ä–∞–±–æ—Ç—É', '—Ä–∞–±–æ—Ç–∞'),\n",
       "  ('.', '.'),\n",
       "  ('–ó–Ω–∞—é', '–∑–Ω–∞—Ç—å'),\n",
       "  ('.', '.')],\n",
       " [('–ñ–µ–ª–µ–∑–Ω—ã–π', '–∂–µ–ª–µ–∑–Ω—ã–π'), ('–∞—Ä–≥—É–º–µ–Ω—Ç', '–∞—Ä–≥—É–º–µ–Ω—Ç'), ('.', '.')],\n",
       " [('–¢', '—Ç–∞–∫'),\n",
       "  ('.', '.'),\n",
       "  ('–µ', '–±—ã—Ç—å'),\n",
       "  ('.', '.'),\n",
       "  ('–Ω–∞–∑–ª–æ', '–Ω–∞–∑–ª–æ'),\n",
       "  ('–º–∞–º–µ', '–º–∞–º–∞'),\n",
       "  ('–æ—Ç–º–æ—Ä–æ–∂—É', '–æ—Ç–º–æ—Ä–æ–∑–∏—Ç—å'),\n",
       "  ('—É—à–∏', '—É—Ö–æ'),\n",
       "  ('?', '?')]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsedrussian[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "'''–ó–∞–¥–∞–¥–∏–º —Å–ª–æ–≤–æ, –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å –∫–æ—Ç–æ—Ä—ã–º –º—ã —Ö–æ—Ç–∏–º –æ—Ç—Å–µ—è—Ç—å'''\n",
    "\n",
    "keyword = '–¥–µ–ª–∞—Ç—å'  # –º–µ–Ω—è–π—Ç–µ –Ω–∞ –Ω—É–∂–Ω–æ–µ —Å–ª–æ–≤–æ. –≠—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –∏–ª–∏ —Ç–æ–∫–µ–Ω, –∏–ª–∏ –ª–µ–º–º–∞ (–¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ –≤–ø–æ–ª–Ω–µ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–Ω—Ñ–∏–Ω–∏—Ç–∏–≤—ã)\n",
    "filteredrussian = []\n",
    "for tweet in parsedrussian:\n",
    "    for token in tweet:\n",
    "        if keyword in token:\n",
    "            filteredrussian.append(tweet)\n",
    "            break\n",
    "print(len(filteredrussian))  # –ø–æ—Å–º–æ—Ç—Ä–∏–º, —Å–∫–æ–ª—å–∫–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π —É –Ω–∞—Å –ø–æ–ª—É—á–∏–ª–æ—Å—å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''–ó–∞–ø–∏—à–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤ xlsx'''\n",
    "\n",
    "d = dict(enumerate(filteredrussian))\n",
    "dict_df = pd.DataFrame({key:pd.Series(value) for key, value in d.items()}).T\n",
    "dict_df.to_excel('russian.xlsx')  # –ú–µ–Ω—è–π—Ç–µ –Ω–∞–∑–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "11aeeef34c643d6c51503798d848dcb599554d380fb974c43f33a485e5ca305f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
